DALLE_CFG:
  # DALLE_CFG.VOCAB_SIZE: tokenizer.vocab_size  # refer to EDA, there are only 333 words total. but input_ids index should be in within 0 ~ 52000: https://github.com/boostcampaitech2-happyface/DALLE-Couture/blob/pytorch-dalle/EDA.ipynb
  DALLE_PATH: None # training from scratch
  TAMING: True # use VAE from taming transformers paper
  BPE_PATH: None
  RESUME: exists(DALLE_PATH)

  EPOCHS: 5
  BATCH_SIZE: 24

  # configuration mimics of: https://github.com/lucidrains/DALLE-pytorch/discussions/131
  # Hyperparameter testing on pytorch-dalle: https://github.com/lucidrains/DALLE-pytorch/issues/84
  # Another Reference for Hyperparams https://github.com/lucidrains/DALLE-pytorch/issues/86#issue-832121328
  LEARNING_RATE: 0.0003
  GRAD_CLIP_NORM: 0.5

  TEXT_SEQ_LEN: 64
  DEPTH: 16 # 16 layers of transformers
  HEADS: 8
  DIM_HEAD: 64 # 8개의 head, 64는 각 head의 dimension
  REVERSIBLE: True
  LOSS_IMG_WEIGHT: 7
  ATTN_TYPES: "full"
  FF_DROPOUT: 0.2 # Feed forward dropout
  ATTN_DROPOUT: 0.2 # Attention Feed forward dropout
  STABLE: None # stable_softmax
  SHIFT_TOKENS: None
  ROTARY_EMB: None

  resize_ratio: 1.0
  truncate_captions: True
