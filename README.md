# KoDALLE

[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Live%20Demo%20on%20Spaces-blue.svg)](https://huggingface.co/spaces/nateraw/spaces-template-gradio) [![Wandb Log](https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-gradient.svg)](https://wandb.ai/happyface-boostcamp/final)

![image-20211227151557604](assets/README/image-20211227151557604.png)

**Utilizing pretrained language modelâ€™s token embedding layer and position embedding layer as DALLEâ€™s text encoder.**

### Background

- Training DALLE model from scratch demands large size paired dataset of images and captions. For example, OpenAI DALLE is trained with more than 250 million text-image pairs for the training.
- If the dataset isnâ€™t large enough or is limited to specific domains, number of vocabularies in the trained DALLE model are insufficient. For instance, 1 million text captions of K-Fashion dataset only consists of more or less than 300 tokens.
- Therefore, inferencing from such DALLE models could be problematic if the given sentence query is unconnected to the originally trained captionsâ€™ text dataset.

### KoDALLE's Result on Small Size Fashion Dataset

|                        |      OpenAIâ€™s DALLE       |           KoDALLE of HappyFace            |
| :--------------------: | :-----------------------: | :---------------------------------------: |
| **Train Dataset Size** |     250 Million Pairs     |             0.8 Million Pairs             |
|      **#Params**       |        12 Billion         |                428 Million                |
|      **#Layers**       |         64 Layers         |                 16 Layers                 |
| **Computing Resource** |     1024 x V100 16GB      |               1 x V100 32GB               |
|    **Text Encoder**    | 16384 Vocab x 512 Dim BPE | 32000 Vocab x 1024 Dim klue/roberta-large |
|   **Image Encoder**    |           VQVAE           |                   VQGAN                   |
|     **Optimizer**      |           AdamW           |                   AdamW                   |
|   **Learning Rate**    |          4.5e-5           |                  3.0e-5                   |
|    **Weight Decay**    |          4.5e-3           |                  3.0e-3                   |
|    **LR Scheduler**    |     ReduceLROnPlateau     |                     -                     |

**The team constructed Text to Fashion Design DALLE model in Korean language with less than 100k text-image sampled pairs.**

|                     |                                                              |
| :-----------------: | :----------------------------------------------------------: |
|     **Caption**     | í•˜ì˜ì—ì„œ ìƒ‰ìƒì€ ìŠ¤ì¹´ì´ë¸”ë£¨ì´ë‹¤. ìƒì˜ì—ì„œ ê¸°ì¥ì€ ë¡±ì´ë‹¤. ìƒ‰ìƒì€ í™”ì´íŠ¸ì´ë‹¤. ì¹´í…Œê³ ë¦¬ëŠ” ë¸”ë¼ìš°ìŠ¤ì´ë‹¤. ë””í…Œì¼ì—ëŠ” ì…”ë§ì´ë‹¤. ì†Œë§¤ê¸°ì¥ì€ ë°˜íŒ”ì´ë‹¤. ì†Œì¬ì—ëŠ” ì‹¤í¬ì´ë‹¤. í”„ë¦°íŠ¸ì—ëŠ” ë¬´ì§€ì´ë‹¤. ë„¥ë¼ì¸ì€ ë¸Œì´ë„¥ì´ë‹¤. í•ì€ ë…¸ë©€ |
| **Generated Image** | <img height="250" width="200" alt="image" src="assets/README/image-20211227152252313.png"> |

|                     |                                                              |
| :-----------------: | :----------------------------------------------------------: |
|     **Caption**     | ì•„ìš°í„°ëŠ” ìƒ‰ìƒì´ ì¹´í‚¤ ì†Œì¬ê°€ ìš°ë¸ í•ì´ ë£¨ì¦ˆì¸ ì½”íŠ¸ì´ë‹¤. í•˜ì˜ëŠ” ìƒ‰ìƒì´ ë„¤ì´ë¹„ ì†Œì¬ê°€ ë°ë‹˜ í•ì´ ìŠ¤í‚¤ë‹ˆì¸ ì²­ë°”ì§€ì´ë‹¤. |
| **Generated Image** | <img height="250" width="200" alt="image" src="assets/README/image-20211227152034538.png"> |

|                     |                                                              |
| :-----------------: | :----------------------------------------------------------: |
|     **Caption**     | í•˜ì˜ì—ì„œ ê¸°ì¥ì€ ë°œëª©ì´ë‹¤. ìƒ‰ìƒì€ ë¸”ë£¨ì´ë‹¤. ì¹´í…Œê³ ë¦¬ëŠ” ìŠ¤ì»¤íŠ¸ì´ë‹¤. ì†Œì¬ì—ëŠ” ë°ë‹˜ì´ë‹¤. í•ì€ ì™€ì´ë“œì´ë‹¤. ìƒì˜ì—ì„œ ìƒ‰ìƒì€ í™”ì´íŠ¸ì´ë‹¤. ì¹´í…Œê³ ë¦¬ëŠ” ë¸”ë¼ìš°ìŠ¤ì´ë‹¤. ë””í…Œì¼ì—ëŠ” ì…”ë§ì´ë‹¤. ì†Œë§¤ê¸°ì¥ì€ ë°˜íŒ”ì´ë‹¤. ì†Œì¬ì—ëŠ” ìš°ë¸ì´ë‹¤. |
| **Generated Image** | <img height="250" width="200" alt="image" src="assets/README/image-20211227152127324.png"> |

|                     |                                                              |
| :-----------------: | :----------------------------------------------------------: |
|     **Caption**     | ìƒì˜ì—ì„œ ê¸°ì¥ì€ ë…¸ë©€ì´ë‹¤. ìƒì˜ì—ì„œ ìƒ‰ìƒì€ í™”ì´íŠ¸ì´ë‹¤. ìƒì˜ì—ì„œ ì„œë¸Œìƒ‰ìƒì€ ë¸”ë™ì´ë‹¤. ìƒì˜ì—ì„œ ì¹´í…Œê³ ë¦¬ëŠ” í‹°ì…”ì¸ ì´ë‹¤. ìƒì˜ì—ì„œ ì†Œë§¤ê¸°ì¥ì€ ë°˜íŒ”ì´ë‹¤. ìƒì˜ì—ì„œ ì†Œì¬ì—ëŠ” ì €ì§€ì´ë‹¤. ìƒì˜ì—ì„œ í”„ë¦°íŠ¸ì—ëŠ” ë ˆí„°ë§ì´ë‹¤. ìƒì˜ì—ì„œ ë„¥ë¼ì¸ì€ ë¼ìš´ë“œë„¥ì´ë‹¤. ìƒì˜ì—ì„œ í•ì€ ë£¨ì¦ˆì´ë‹¤. |
| **Generated Image** | <img height="250" width="200" alt="image" src="assets/README/image-20211227152337621.png"> |

### Methodology

Experimentations were conducted with the following Korean Transformers Modelsâ€™ embedding layers. The team selected klue/roberta-large as baseline in the repository considering the size of the model.

- **[klue/roberta-large](https://huggingface.co/klue/roberta-large): Vocab Size of 32000, Embedding Dimension of 1024.**
- [KoGPT Trinity of SKT](https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5): Vocab Size of 51200, Embedding Dimension of 1920.
- [KoGPT of Kakao Brain](https://huggingface.co/kakaobrain/kogpt): Vocab Size of 64512, Embedding Dimension of 4096.

KoDALLE with klue/roberta-large's wpe and wte were trained on 32GB V100 GPU environment. Hyperparams related to the DALLE's model size are following.

```
'BATCH_SIZE': 40
'DEPTH': 16
'TEXT_SEQ_LEN': 128
'VOCAB_SIZE': 32000
'MODEL_DIM': 1024
'ATTN_TYPES': 'full'
'DIM_HEAD': 64
'HEADS': 8
```

- DALLE model is composed on [lucidrain's DALLE-pytorch](https://github.com/lucidrains/DALLE-pytorch)
- Image encoder is constructed based on [VQGAN(Taming Transformers)](https://github.com/CompVis/taming-transformers#training-on-custom-data)

### Significance

- Offers promising result for training from scratch on specific domains with small size dataset.
- Introduces solution for domain specific DALLE & CLIP models to be robust on input sentence.
- Recommends adequate text-to-image model size for given computation resource.
- Suggests effortless method of creating DALLE & CLIP model for own languages if pretrained language model is available.

---

### WIP

- [x] Add image-caption reranker(EfficientNet + Klue/roberta-large)
- [x] Model trained with 500k text-image pairs.
- [x] Modulize in python code.
- [x] Update Inference code.
- [ ] Update FID and IS metrics on test and validation dataset.
